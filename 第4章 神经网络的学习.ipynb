{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章 神经网络的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章的主题是神经网络的学习。这里所说的“学习”是指从训练数据中 自动获取最优权重参数的过程。本章中，为了使神经网络能进行学习，将导 入损失函数这一指标。而学习的目的就是以该损失函数为基准，找出能使它 的值达到最小的权重参数。为了找出尽可能小的损失函数的值，本章我们将 介绍利用了函数斜率的梯度法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 从数据中学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指 可以由数据自动决定权重参数的值。这是非常了不起的事情！因为如果所有 的参数都需要人工决定的话，工作量就太大了。在第2章介绍的感知机的例 子中，我们对照着真值表，人工设定了参数的值，但是那时的参数只有3个。 而在实际的神经网络中，参数的数量成千上万，在层数更深的深度学习中， 参数的数量甚至可以上亿，想要人工决定这些参数的值是不可能的。本章将 介绍神经网络的学习，即利用数据决定参数值的方法，并用Python实现对 MNIST手写数字数据集的学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1　数据驱动\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据是机器学习的命根子。从数据中寻找答案、从数据中发现模式、根 据数据讲故事……这些机器学习所做的事情，如果没有数据的话，就无从谈 起。因此，数据是机器学习的核心。这种数据驱动的方法，也可以说脱离了 过往以人为中心的方法。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果让我们自己来设计一个能将5正确分类的程序，就会意外地发现这 是一个很难的问题。人可以简单地识别出5，但却很难明确说出是基于何种 规律而识别出了5。此外，从图4-1中也可以看到，每个人都有不同的写字习惯， 要发现其中的规律是一件非常难的工作。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，与其绞尽脑汁，从零开始想出一个可以识别5的算法，不如考虑 通过有效利用数据来解决这个问题。一种方案是，先从图像中提取特征再用机器学习技术学习这些特征量的模式。这里所说的“特征量”是指可以 从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。图 像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括 SIFT、SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对 转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。 量，再用机器学习技术学习这些特征量的模式。这里所说的“特征量”是指可以 从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。图 像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括 SIFT、SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对 转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习有时也称为端到端机器学习（end-to-end machine  learning）。这里所说的端到端是指从一端到另一端的意思，也就是 从原始数据（输入）中获得目标结果（输出）的意思。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的优点是对所有的问题都可以用同样的流程来解决。比如，不 管要求解的问题是识别5，还是识别狗，抑或是识别人脸，神经网络都是通 过不断地学习所提供的数据，尝试发现待求解的问题的模式。也就是说，与 待处理的问题无关，神经网络可以将数据直接作为原始数据，进行“端对端” 的学习。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2　训练数据和测试数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和 实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试 数据评价训练得到的模型的实际能力。为什么需要将数据分为训练数据和测 试数据呢？因为我们追求的是模型的泛化能力。为了正确评价模型的泛化能 力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 损失函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均方误差和交叉熵误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1　均方误差\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y,t):\n",
    "    return 0.5*np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设“2”为正确解 \n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例1：“2”的概率最高的情况（0.6）\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例2：“7”的概率最高的情况（0.6） \n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] \n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里举了两个例子。第一个例子中，正确解是“2”，神经网络的输出的最大 值是“ 2”；第二个例子中，正确解是“2”，神经网络的输出的最大值是“7”。如 实验结果所示，我们发现第一个例子的损失函数的值更小，和监督数据之间的 误差较小。也就是说，均方误差显示第一个例子的输出结果与监督数据更加吻合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2　交叉熵误差\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，参数y和t是NumPy数组。函数内部在计算np.log时，加上了一 个微小值delta。这是因为，当出现np.log(0)时，np.log(0)会变为负无限大 的-inf，这样一来就会导致后续计算无法进行。作为保护性对策，添加一个 微小值可以防止负无限大的发生。下面，我们使用cross_entropy_error(y, t) 进行一些简单的计算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] \n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y), np.array(t)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个例子中，正确解标签对应的输出为0.6，此时的交叉熵误差大约 为0.51。第二个例子中，正确解标签对应的输出为0.1的低值，此时的交叉 熵误差大约为2.3。由此可以看出，这些结果与我们前面讨论的内容是一致的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3　mini-batch学习\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "，MNIST数据集的训练数据有60000个，如果以全部数据为对象 求损失函数的和，则计算过程需要花费较长的时间。再者，如果遇到大数据， 数据量会有几百万、几千万之多，这种情况下以全部数据为对象计算损失函 数是不现实的。因此，我们从全部数据中选出一部分，作为全部数据的“近 似”。神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小 批量），然后对每个mini-batch进行学习。比如，从60000个训练数据中随机 选择100笔，再用这100笔数据进行学习。这种学习方式称为mini-batch学习。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来编写从训练数据中随机选择指定个数的数据的代码，以进行 mini-batch学习。在这之前，先来看一下用于读入MNIST数据集的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train,t_train),(x_test,t_test) = \\\n",
    "load_mnist(normalize = True,one_hot_label = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第3章介绍过，load_mnist函数是用于读入MNIST数据集的函数。这个 函数在本书提供的脚本dataset/mnist.py中，它会读入训练数据和测试数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入数据时，通过设定参数one_hot_label=True，可以得到one-hot表示（即 仅正确解标签为1，其余为0的数据结构）。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入上面的MNIST数据后，训练数据有60000个，输入数据是784维 （28×28）的图像数据，监督数据是10维的数据。因此，上面的x_train、t_ train的形状分别是(60000, 784)和(60000, 10)。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，如何从这个训练数据中随机抽取10笔数据呢？我们可以使用 NumPy的np.random.choice()，写成如下形式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size,batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用np.random.choice()可以从指定的数字中随机选择想要的数字。比如， np.random.choice(60000, 10)会从0到59999之间随机选择10个数字。如下 面的实际代码所示，我们可以得到一个包含被选数据的索引的数组。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1754, 5814, 4245, 2591, 4568, 5331, 4271,  891,  836,  308])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(6000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后，我们只需指定这些随机选出的索引，取出mini-batch，然后使用 这个mini-batch计算损失函数即可。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算电视收视率时，并不会统计所有家庭的电视机，而是仅以那些 被选中的家庭为统计对象。比如，通过从关东地区随机选择1000个 家庭计算收视率，可以近似地求得关东地区整体的收视率。这1000 个家庭的收视率，虽然严格上不等于整体的收视率，但可以作为整 体的一个近似值。和收视率一样，mini-batch的损失函数也是利用 一部分样本数据来近似地计算整体。也就是说，用随机选择的小批 量数据（mini-batch）作为全体训练数据的近似值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4　mini-batch版交叉熵误差的实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何实现对应mini-batch的交叉熵误差呢？只要改良一下之前实现的对 应单个数据的交叉熵误差就可以了。这里，我们来实现一个可以同时处理单 个数据和批量数据（数据作为batch集中输入）两种情况的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，y是神经网络的输出，t是监督数据。y的维度为1时，即求单个 数据的交叉熵误差时，需要改变数据的形状。并且，当输入为mini-batch时， 要用batch的个数进行正规化，计算单个数据的平均交叉熵误差。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5　为何要设定损失函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 数值微分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度法使用梯度的信息决定前进的方向。本节将介绍梯度是什么、有什 么性质等内容。在这之前，我们先来介绍一下导数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1　导数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们参考式（4.4），来实现求函数的导数的程序。如果直接实 现式（4.4）的话，向h中赋入一个微小值，就可以计算出来了。比如，下面 的实现如何？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 10e-50\n",
    "    return (f(x+h) - f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数numerical_diff(f, x)的名称来源于数值微分A 的英文numerical differentiation。这个函数有两个参数，即“函数f”和“传给函数f的参数x”。 乍一看这个实现没有问题，但是实际上这段代码有两处需要改进的地方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的实现中，因为想把尽可能小的值赋给h（可以话，想让h无限 接近0），所以 h使用了10e-50这个微小值。但是，这样反而产生了舍入误差 （rounding error）。所谓舍入误差，是指因省略小数的精细部分的数值（比如， 小数点后第8位以后的数值）而造成最终的计算结果上的误差。比如，在 Python中，舍入误差可如下表示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示，如果用float32类型（32位的浮点数）来表示1e-50，就会变成 0.0，无法正确表示出来。也就是说，使用过小的值会造成计算机出现计算 上的问题。这是第一个需要改进的地方，即将微小值h改为10−4。使用10−4 就可以得到正确的结果。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个需要改进的地方与函数f的差分有关。虽然上述实现中计算了函 数f在x+h和x之间的差分，但是必须注意到，这个计算从一开始就有误差。 如图4-5所示，“真的导数”对应函数在x处的斜率（称为切线），但上述实现 中计算的导数对应的是(x + h)和x之间的斜率。因此，真的导数（真的切线） 和上述实现中得到的导数的值在严格意义上并不一致。这个差异的出现是因 为h不可能无限接近0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图4-5所示，数值微分含有误差。为了减小这个误差，我们可以计算 函数f在(x + h)和(x−h)之间的差分。因为这种计算方法以x为中心，计 算它左右两边的差分，所以也称为中心差分（而 (x + h)和x之间的差分称为 前向差分）。下面，我们基于上述两个要改进的点来实现数值微分（数值梯度）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示，利用微小的差分求导数的过程称为数值微分（numerical  differentiation）。而基于数学式的推导求导数的过程，则用“解析 性”（analytic）一词，称为“解析性求解”或者“解析性求导”。比如， y = x2的导数，可以通过 解析性地求解出来。因此，当x = 2时， y的导数为4。解析性求导得到的导数是不含误差的“真的导数”。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2　数值微分的例子\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们试着用上述的数值微分对简单函数进行求导。先来看一个由下 式表示的2次函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = 0.01x2 + 0.1x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们来绘制这个函数的图像。画图所用的代码如下，生成的图 像如图4-6所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0,20.0,0.1) # 以0.1为单位，从0到20的数组x \n",
    "y = function(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel('y')\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来计算一下这个函数在x = 5和x = 10处的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里计算的导数是f(x)相对于x的变化量，对应函数的斜率。另外， f(x) = 0.01x2 + 0.1x的解析解是 。因 此，在x = 5和 x = 10处，“真的导数”分别为0.2和0.3。和上面的结果相比，我们发现虽然 严格意义上它们并不一致，但误差非常小。实际上，误差小到基本上可以认 为它们是相等的。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tangent_line(f,x):\n",
    "    d = numerical_diff(f,x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t:d*t +y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n"
     ]
    }
   ],
   "source": [
    "tf = tangent_line(function, 5)\n",
    "y2 = tf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3　偏导数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们看一下式(4.6)表示的函数。虽然它只是一个计算参数的 平方和的简单函数，但是请注意和上例不同的是，这里有两个变量。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题1：求x0 = 3,x 1 = 4时，关于x0的偏导数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_tmp1,3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题2：求x0 = 3,x 1 = 4时，关于x1的偏导数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_tmp2,4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这些问题中，我们定义了一个只有一个变量的函数，并对这个函数进 行了求导。例如，问题1中，我们定义了一个固定x1 = 4的新函数，然后对 只有变量x0的函数应用了求数值微分的函数。从上面的计算结果可知，问题 1的答案是6.00000000000378，问题2的答案是7.999999999999119，和解析 解的导数基本一致。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像这样，偏导数和单变量的导数一样，都是求某个地方的斜率。不过， 偏导数需要将多个变量中的某一个变量定为目标变量，并将其他变量固定为 某个值。在上例的代码中，为了将目标变量以外的变量固定到某些特定的值 上，我们定义了新函数。然后，对新定义的函数应用了之前的求数值微分的 函数，得到偏导数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在刚才的例子中，我们按变量分别计算了x0和x1的偏导数。现在，我 们希望一起计算x0和x1的偏导数。比如，我们来考虑求x0 = 3,x 1 = 4时(x0,x 1) 的偏导数 。另外，像 这样的由全部变量的偏导数汇总 而成的向量称为梯度（gradient）。梯度可以像下面这样来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        #f(x+h)的计算\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        #f(x-h)的计算\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2)/(2*h)\n",
    "        x[idx] = tmp_val#还原值\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数numerical_gradient(f, x)的实现看上去有些复杂，但它执行的处 理和求单变量的数值微分基本没有区别。需要补充说明一下的是，np.zeros_ like(x)会生成一个形状和x相同、所有元素都为0的数组。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数numerical_gradient(f, x)中，参数f为函数，x为NumPy数组，该 函数对NumPy数组x的各个元素求数值微分。现在，我们用这个函数实际 计算一下梯度。这里我们求点(3, 4)、(0,2)、(3,0)处的梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2,np.array([3.0,4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像这样，我们可以计算(x0,x 1)在各点处的梯度。上例中，点(3,4)处的 梯度是(6,8)，点(0, 2)处的梯度是(0, 4)，点(3,0)处的梯度是(6,0)。这个 梯度意味着什么呢？为了更好地理解，我们把 的梯度画 在图上。不过，这里我们画的是元素值为负梯度B 的向量（"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1　梯度法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必 须在学习时找到最优参数（权重和偏置）。这里所说的最优参数是指损失函数\n",
    "取最小值时的参数。但是，一般而言，损失函数很复杂，参数空间庞大，我 们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值 （或者尽可能小的值）的方法就是梯度法。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里需要注意的是，梯度表示的是各点处的函数值减小最多的方向。因此， 无法保证梯度所指的方向就是函数的最小值或者真正应该前进的方向。实际 上，在复杂的函数中，梯度指示的方向基本上都不是函数值最小处。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数的极小值、最小值以及被称为鞍点（saddle point）的地方， 梯度为0。极小值是局部最小值，也就是限定在某个范围内的最 小值。鞍点是从某个方向上看是极大值，从另一个方向上看则是 极小值的点。虽然梯度法是要寻找梯度为0的地方，但是那个地 方不一定就是最小值（也有可能是极小值或者鞍点）。此外，当函 数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区， 陷入被称为“学习高原”的无法前进的停滞期。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然梯度的方向并不一定指向最小值，但沿着它的方向能够最大限度地 减小函数的值。因此，在寻找函数的最小值（或者尽可能小的值）的位置的 任务中，要以梯度的信息为线索，决定前进的方向。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时梯度法就派上用场了。在梯度法中，函数的取值从当前位置沿着梯 度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进， 如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度方向前进， 逐渐减小函数值的过程就是梯度法（gradient method）。梯度法是解决机器 学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式（4.7）的 η表示更新量，在神经网络的学习中，称为学习率（learning rate）。学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f,init_x,lr = 0.01,step_num = 100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr*grad\n",
    "        \n",
    "    return x,np.array(x_history)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数f是要进行最优化的函数，init_x是初始值，lr是学习率learning rate，step_num是梯度法的重复次数。numerical_gradient(f,x)会求函数的 梯度，用该梯度乘以学习率得到的值进行更新操作，由step_num指定重复的 次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用这个函数可以求函数的极小值，顺利的话，还可以求函数的最小值。 下面，我们就来尝试解决下面这个问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题：请用梯度法求 的最小值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0,4.0])\n",
    "gradient_descent(function_2,init_x = init_x,lr = 0.1,step_num = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，设初始值为(-3.0, 4.0)，开始使用梯度法寻找最小值。最终的结 果是(-6.1e-10, 8.1e-10)，非常接近(0，0)。实际上，真的最小值就是(0，0)， 所以说通过梯度法我们基本得到了正确结果。如果用图来表示梯度法的更新 过程，则如图4-10所示。可以发现，原点处是最低的地方，函数的取值一 点点在向其靠近。这个图的源代码在ch04/gradient_method.py 中（但 ch04/ gradient_method.py不显示表示等高线的虚线）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面说过，学习率过大或者过小都无法得到好的结果。我们来做个实验 验证一下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.58983747e+13, -1.29524862e+12]),\n",
       " array([[-3.00000000e+00,  4.00000000e+00],\n",
       "        [ 5.70000000e+01, -7.60000000e+01],\n",
       "        [-1.08300000e+03,  1.44400000e+03],\n",
       "        [ 2.05770000e+04, -2.74360000e+04],\n",
       "        [-3.90963008e+05,  5.21284002e+05],\n",
       "        [ 7.42829664e+06, -9.90439654e+06],\n",
       "        [-1.41137328e+08,  1.88183103e+08],\n",
       "        [ 2.68126267e+09, -3.57501690e+09],\n",
       "        [-5.09763373e+10,  6.79001831e+10],\n",
       "        [ 9.45170863e+11, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12],\n",
       "        [-2.58983747e+13, -1.29524862e+12]]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学习率过大的例子：lr=10.0 \n",
    "init_x = np.array([-3.0, 4.0]) \n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.99999994,  3.99999992]), array([[-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  4.        ],\n",
       "        [-3.        ,  3.99999999],\n",
       "        [-3.        ,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999999],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999999,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999998],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999998,  3.99999997],\n",
       "        [-2.99999997,  3.99999997],\n",
       "        [-2.99999997,  3.99999997],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999996],\n",
       "        [-2.99999997,  3.99999995],\n",
       "        [-2.99999997,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999995],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999996,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999994],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999995,  3.99999993],\n",
       "        [-2.99999994,  3.99999993],\n",
       "        [-2.99999994,  3.99999993],\n",
       "        [-2.99999994,  3.99999992],\n",
       "        [-2.99999994,  3.99999992],\n",
       "        [-2.99999994,  3.99999992],\n",
       "        [-2.99999994,  3.99999992],\n",
       "        [-2.99999994,  3.99999992],\n",
       "        [-2.99999994,  3.99999992]]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学习率过小的例子：lr=1e-10 \n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验结果表明，学习率过大的话，会发散成一个很大的值；反过来，学 习率过小的话，基本上没怎么更新就结束了。也就是说，设定合适的学习率 是一个很重要的问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像学习率这样的参数称为超参数。这是一种和神经网络的参数（权重 和偏置）性质不同的参数。相对于神经网络的权重参数是通过训练 数据和学习算法自动获得的，学习率这样的超参数则是人工设定的。 一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利 进行的设定。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2　神经网络的梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们以一个简单的神经网络为例，来实现求梯度的代码。为此， 我们要实现一个名为simpleNet的类（源代码在ch04/gradient_simplenet.py 中）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax,cross_entropy_error\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)#利用高斯分布进行初始化\n",
    "    \n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用了common/functions.py中的softmax和cross_entropy_error方 法，以及common/gradient.py中的numerical_gradient方法。simpleNet类只有 一个实例变量，即形状为2×3的权重参数。它有两个方法，一个是用于预 测的predict(x)，另一个是用于求损失函数值的loss(x,t)。这里参数x接收 输入数据，t接收正确解标签。现在我们来试着用一下这个simpleNet。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = simpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22220423 -2.1791881   2.41727075]\n",
      " [ 0.15516256  1.01782958 -0.21539544]]\n"
     ]
    }
   ],
   "source": [
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.87296884, -0.39146624,  1.25650655])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6,0.9])\n",
    "p = net.predict(x)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6280141657077565"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来求梯度。和前面一样，我们使用numerical_gradient(f, x)求梯 度（这里定义的函数f(W)的参数W是一个伪参数。因为numerical_gradient(f, x)会在内部执行f(x),为了与之兼容而定义了f(W)）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = numerical_gradient(f,net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21819244,  0.06161728, -0.27980971],\n",
       "       [ 0.32728865,  0.09242592, -0.41971457]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical_gradient(f, x) 的参数f是函数，x是传给函数f的参数。因此， 这里参数x取net.W，并定义一个计算损失函数的新函数f，然后把这个新定 义的函数传递给numerical_gradient(f, x)。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical_gradient(f, net.W)的结果是dW，一个形状为2×3的二维数组。 观察一下dW的内容，例如，会发现 中的 的值大约是0.2，这表示如 果将w11增加h，那么损失函数的值会增加0.2h。再如， 对应的值大约 是−0.5，这表示如果将w23增加h，损失函数的值将减小0.5h。因此，从减 小损失函数值的观点来看，w23应向正方向更新，w11应向负方向更新。至于 更新的程度，w23比w11的贡献要大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，在上面的代码中，定义新函数时使用了“def f(x):···”的形式。 实际上，Python中如果定义的是简单的函数，可以使用lambda表示法。使 用lambda的情况下，上述代码可以如下实现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda W: net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21819244,  0.06161728, -0.27980971],\n",
       "       [ 0.32728865,  0.09242592, -0.41971457]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW = numerical_gradient(f,net.W)\n",
    "dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可。 在下一节中，我们会以2层神经网络为例，实现整个学习过程。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了对应形状为多维数组的权重参数W，这里使用的numerical_ gradient()和之前的实现稍有不同。不过，改动只是为了对应多维 数组，所以改动并不大。这里省略了对代码的说明，想知道细节的 读者请参考源代码（common/gradient.py）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 学习算法的实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于神经网络学习的基础知识，到这里就全部介绍完了。“损失函 数”“ mini-batch”“梯度”“梯度下降法”等关键词已经陆续登场，这里我们 来确认一下神经网络的学习步骤，顺便复习一下这些内容。神经网络的学习 步骤如下所示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前提 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的 过程称为“学习”。神经网络的学习分成下面4个步骤。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步骤1（mini-batch） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们 的目标是减小mini-batch的损失函数的值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步骤2（计算梯度） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。 梯度表示损失函数的值减小最多的方向。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步骤3（更新参数） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将权重参数沿梯度方向进行微小更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步骤4（重复） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重复步骤1、步骤2、步骤3。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的学习按照上面4个步骤进行。这个方法通过梯度下降法更新 参数，不过因为这里使用的数据是随机选择的mini batch数据，所以又称为 随机梯度下降法（stochastic gradient descent）。“随机”指的是“随机选择的” 的意思，因此，随机梯度下降法是“对随机选择的数据进行的梯度下降法”。 深度学习的很多框架中，随机梯度下降法一般由一个名为SGD的函数来实现。 SGD来源于随机梯度下降法的英文名称的首字母。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们来实现手写数字识别的神经网络。这里以2层神经网络（隐 藏层为1层的网络）为对象，使用MNIST数据集进行学习。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1　2层神经网络的类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m首先，我们将这个2层神经网络实现为一个名为TwoLayerNet的类，实现 过程如下所示A 。源代码在ch04/two_layer_net.py中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,\n",
    "                weight_init_std = 0.01):\n",
    "        #初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std*\\\n",
    "                            np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std*\\\n",
    "                            np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        W1,W2 = self.params['W1'],self.params['W2']\n",
    "        b1,b2 = self.params['b1'],self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x,W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    #x:输入数据, t:监督数据 \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y,t)\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = no,argmax(y,axis=1)\n",
    "        t = np.argmax(t,axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "     # x:输入数据, t:监督数据 \n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W:self.loss(x,t)\n",
    "        grads = {}       \n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])        \n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])        \n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])        \n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然这个类的实现稍微有点长，但是因为和上一章的神经网络的前向处 理的实现有许多共通之处，所以并没有太多新东西。我们先把这个类中用到 的变量和方法整理一下。表4-1中只罗列了重要的变量，表4-2中则罗列了所 有的方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet类有params和grads两个字典型实例变量。params变量中保存 了权重参数，比如params['W1']以NumPy数组的形式保存了第1层的权重参 数。此外，第1层的偏置可以通过param['b1']进行访问。这里来看一个例子。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10) \n",
    "net.params['W1'].shape # (784, 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['b1'].shape # (100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['W2'].shape # (100, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['b2'].shape # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示，params变量中保存了该神经网络所需的全部参数。并且， params变量中保存的权重参数会用在推理处理（前向处理）中。顺便说一下， 推理处理的实现如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784) # 伪输入数据（100笔） \n",
    "y = net.predict(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，与 params变量对应，grads变量中保存了各个参数的梯度。如下所示， 使用numerical_gradient()方法计算梯度后，梯度的信息将保存在grads变 量中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100,784)\n",
    "t = np.random.rand(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gards = net.numerical_gradient(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gards['W1'].shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我们来看一下TwoLayerNet的方法的实现。首先是__init__(self, input_size, hidden_size, output_size)方法，它是类的初始化方法（所谓初 始化方法，就是生成TwoLayerNet实例时被调用的方法）。从第1个参数开始， 依次表示输入层的神经元数、隐藏层的神经元数、输出层的神经元数。另外， 因为进行手写数字识别时，输入图像的大小是784（28×28），输出为10个类别， 所以指定参数input_size=784、output_size=10，将隐藏层的个数hidden_size 设置为一个合适的值即可。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，这个初始化方法会对权重参数进行初始化。如何设置权重参数 的初始值这个问题是关系到神经网络能否成功学习的重要问题。后面我 们会详细讨论权重参数的初始化，这里只需要知道，权重使用符合高斯 分布的随机数进行初始化，偏置使用0进行初始化。predict(self, x)和 accuracy(self, x, t)的实现和上一章的神经网络的推理处理基本一样。如 果仍有不明白的地方，请再回顾一下上一章的内容。另外，loss(self, x, t)是计算损失函数值的方法。这个方法会基于predict()的结果和正确解标签， 计算交叉熵误差。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剩下的numerical_gradient(self, x, t)方法会计算各个参数的梯度。根 据数值微分，计算各个参数相对于损失函数的梯度。另外，gradient(self, x, t) 是下一章要实现的方法，该方法使用误差反向传播法高效地计算梯度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical_gradient(self, x, t)基于数值微分计算参数的梯度。下 一章，我们会介绍一个高速计算梯度的方法，称为误差反向传播法。 用误差反向传播法求到的梯度和数值微分的结果基本一致，但可以 高速地进行处理。使用误差反向传播法计算梯度的gradient(self, x, t)方法会在下一章实现，不过考虑到神经网络的学习比较花时间， 想节约学习时间的读者可以替换掉这里的numerical_gradient(self, x, t)，抢先使用gradient(self, x, t)！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2　mini-batch的实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的学习的实现使用的是前面介绍过的mini-batch学习。所谓 mini-batch学习，就是从训练数据中随机选择一部分数据（称为mini-batch）， 再以这些mini-batch为对象，使用梯度法更新参数的过程。下面，我们就以 TwoLayerNet类为对象，使用MNIST数据集进行学习（源代码在ch04/train_ neuralnet.py中）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from ch04.two_layer_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize = True,one_hot_label = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size = 784,hidden_size = 50,output_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iters_num):\n",
    "    #获取mini_batch\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #计算梯度\n",
    "    grad = network.gradient(x_batch,t_batch)\n",
    "    \n",
    "    #更新参数\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key] -= learning_rate*grad[key]\n",
    "        \n",
    "    #记录学习过程\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，mini-batch的大小为100，需要每次从60000个训练数据中随机 取出100个数据（图像数据和正确解标签数据）。然后，对这个包含100笔数 据的mini-batch求梯度，使用随机梯度下降法（SGD）更新参数。这里，梯 度法的更新次数（循环的次数）为10000。每更新一次，都对训练数据计算损 失函数的值，并把该值添加到数组中。用图像来表示这个损失函数的值的推 移，如图4-11所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(train_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " y = []\n",
    "for i in range(iters_num):\n",
    "    y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,y,'r')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察图4-11，可以发现随着学习的进行，损失函数的值在不断减小。这 是学习正常进行的信号，表示神经网络的权重参数在逐渐拟合数据。也就是 说，神经网络的确在学习！通过反复地向它浇灌（输入）数据，神经网络正 在逐渐向最优参数靠近。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3　基于测试数据的评价\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据图4-11呈现的结果，我们确认了通过反复学习可以使损失函数的值 逐渐减小这一事实。不过这个损失函数的值，严格地讲是“对训练数据的某 个mini-batch的损失函数”的值。训练数据的损失函数值减小，虽说是神经 网络的学习正常进行的一个信号，但光看这个结果还不能说明该神经网络在 其他数据集上也一定能有同等程度的表现。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数 据，即确认是否会发生过拟合。过拟合是指，虽然训练数据中的数字图像能 被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络学习的最初目标是掌握泛化能力，因此，要评价神经网络的泛 化能力，就必须使用不包含在训练数据中的数据。下面的代码在进行学习的 过程中，会定期地对训练数据和测试数据记录识别精度。这里，每经过一个 epoch，我们都会记录下训练数据和测试数据的识别精度。A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch是一个单位。一个epoch表示学习中所有训练数据均被使用过 一次时的更新次数。比如，对于10000笔训练数据，用大小为100 笔数据的mini-batch进行学习时，重复随机梯度下降法100次，所 有的训练数据就都被“看过”了A。此时，100次就是一个epoch。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 实际上，一般做法是事先将所有训练数据随机打乱，然后按指定的批次大小，按序生成mini-batch。 这样每个mini-batch均有一个索引号，比如此例可以是0,1,2,...,99，然后用索引号可以遍历所有 的mini-batch。遍历一次所有数据，就称为一个epoch。请注意，本节中的mini-batch每次都是随机 选择的，所以不一定每个数据都会被看到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc,test acc |0.11236666666666667,0.1135\n",
      "train acc,test acc |0.79855,0.8034\n",
      "train acc,test acc |0.8777,0.8813\n",
      "train acc,test acc |0.8987166666666667,0.9023\n",
      "train acc,test acc |0.9084,0.9106\n",
      "train acc,test acc |0.9140833333333334,0.9181\n",
      "train acc,test acc |0.9199166666666667,0.9221\n",
      "train acc,test acc |0.9236166666666666,0.9267\n",
      "train acc,test acc |0.9271833333333334,0.929\n",
      "train acc,test acc |0.9316,0.9335\n",
      "train acc,test acc |0.9339333333333333,0.9348\n",
      "train acc,test acc |0.9366666666666666,0.9375\n",
      "train acc,test acc |0.9395,0.9403\n",
      "train acc,test acc |0.9414,0.9418\n",
      "train acc,test acc |0.9433333333333334,0.9433\n",
      "train acc,test acc |0.94535,0.9452\n",
      "train acc,test acc |0.94715,0.9457\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from ch04.two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize = True,one_hot_label = True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "#平均每个epoch的重复次数\n",
    "iter_per_epoch = max(train_size/batch_size,1)\n",
    "\n",
    "#超参数\n",
    "iters_num = 10000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size = 784,hidden_size = 50,output_size = 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "     # 获取mini-batch \n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #计算梯度\n",
    "    grad = network.gradient(x_batch,t_batch)\n",
    "    \n",
    "    #更新参数\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key] -= learning_rate*grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    # 计算每个epoch的识别精度\n",
    "    if i%iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('train acc,test acc |' + str(train_acc)+','+str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章中，我们介绍了神经网络的学习。首先，为了能顺利进行神经网络 的学习，我们导入了损失函数这个指标。以这个损失函数为基准，找出使它的值达到最小的权重参数，就是神经网络学习的目标。为了找到尽可能小的 损失函数值，我们介绍了使用函数斜率的梯度法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本章所学的内容 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• 机器学习中使用的数据集分为训练数据和测试数据。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•  神经网络用训练数据进行学习，并用测试数据评价学习到的模型的 泛化能力。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•  神经网络的学习以损失函数为指标，更新权重参数，以使损失函数 的值减小。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• 利用某个给定的微小值的差分求导数的过程，称为数值微分。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• 利用数值微分，可以计算权重参数的梯度。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•  数值微分虽然费时间，但是实现起来很简单。下一章中要实现的稍 微复杂一些的误差反向传播法可以高速地计算梯度。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
