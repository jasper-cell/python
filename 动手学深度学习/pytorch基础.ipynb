{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 创建TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3563e-19, 1.3563e-19, 1.3563e-19],\n",
      "        [1.3563e-19, 1.3563e-19, 1.3563e-19],\n",
      "        [1.3563e-19, 1.3563e-19, 1.3563e-19],\n",
      "        [1.3563e-19, 1.3563e-19, 1.3563e-19],\n",
      "        [7.4086e+28, 1.1080e+27, 1.6907e-01]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3320, 0.6531, 0.0371],\n",
       "        [0.0510, 0.5216, 0.3359],\n",
       "        [0.1716, 0.2196, 0.7359],\n",
       "        [0.2634, 0.9219, 0.4500],\n",
       "        [0.8874, 0.8148, 0.5997]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5,3,dtype = torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5.5,3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5,3,dtype = torch.float64)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5645, -0.5010,  2.0084],\n",
       "        [ 0.5837, -1.0808, -1.3660],\n",
       "        [ 0.1866, -0.3498,  1.3330],\n",
       "        [ 1.3123,  0.9505, -0.4278],\n",
       "        [ 0.3170, -0.1877,  0.7905]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn_like(x,dtype = torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节介绍tensor的各种操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算术操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5680,  0.1010,  2.5338],\n",
       "        [ 1.2850, -0.5814, -0.4363],\n",
       "        [ 0.7093,  0.4492,  1.6771],\n",
       "        [ 2.2748,  1.5493, -0.0091],\n",
       "        [ 0.7623,  0.6533,  1.2320]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5680,  0.1010,  2.5338],\n",
       "        [ 1.2850, -0.5814, -0.4363],\n",
       "        [ 0.7093,  0.4492,  1.6771],\n",
       "        [ 2.2748,  1.5493, -0.0091],\n",
       "        [ 0.7623,  0.6533,  1.2320]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5680,  0.1010,  2.5338],\n",
       "        [ 1.2850, -0.5814, -0.4363],\n",
       "        [ 0.7093,  0.4492,  1.6771],\n",
       "        [ 2.2748,  1.5493, -0.0091],\n",
       "        [ 0.7623,  0.6533,  1.2320]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#可以指定输出\n",
    "result = torch.empty(5,3)\n",
    "torch.add(x,y,out = result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5680,  0.1010,  2.5338],\n",
       "        [ 1.2850, -0.5814, -0.4363],\n",
       "        [ 0.7093,  0.4492,  1.6771],\n",
       "        [ 2.2748,  1.5493, -0.0091],\n",
       "        [ 0.7623,  0.6533,  1.2320]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以使⽤类似NumPy的索引操作来访问 Tensor 的⼀部分，需要注意的是：索引出来的结果与\n",
    "原数据共享内存，也即修改⼀个，另⼀个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5680, 1.1010, 3.5338],\n",
       "        [2.2850, 0.4186, 0.5637],\n",
       "        [1.7093, 1.4492, 2.6771],\n",
       "        [3.2748, 2.5493, 0.9909],\n",
       "        [1.7623, 1.6533, 2.2320]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x[0,:]\n",
    "y += 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5645, -0.5010,  2.0084])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改变形状"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⽤ view() 来改变 Tensor 的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,3,dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.view(15)\n",
    "z = x.view(-1,5)#-1所指的维度可以根据其他维度的值推出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意 view() 返回的新tensor与源tensor共享内存（其实是同⼀个tensor），也即更改其中的⼀个，另\n",
    "外⼀个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察⻆度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5871, 1.8630, 2.7170],\n",
      "        [3.1505, 5.6289, 4.0952],\n",
      "        [3.3564, 2.7228, 2.8769],\n",
      "        [2.6255, 2.6450, 2.9651],\n",
      "        [2.3015, 3.2637, 3.3986]])\n",
      "tensor([3.5871, 1.8630, 2.7170, 3.1505, 5.6289, 4.0952, 3.3564, 2.7228, 2.8769,\n",
      "        2.6255, 2.6450, 2.9651, 2.3015, 3.2637, 3.3986])\n"
     ]
    }
   ],
   "source": [
    "x += 1\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以如果我们想返回⼀个真正新的副本（即不共享内存）该怎么办呢？Pytorch还提供了⼀\n",
    "个 reshape() 可以改变形状，但是此函数并不能保证返回的是其拷⻉，所以不推荐使⽤。推荐先\n",
    "⽤ clone 创造⼀个副本然后再使⽤ view 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cp = x.clone().view(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5871, 0.8630, 1.7170],\n",
      "        [2.1505, 4.6289, 3.0952],\n",
      "        [2.3564, 1.7228, 1.8769],\n",
      "        [1.6255, 1.6450, 1.9651],\n",
      "        [1.3015, 2.2637, 2.3986]])\n",
      "tensor([3.5871, 1.8630, 2.7170, 3.1505, 5.6289, 4.0952, 3.3564, 2.7228, 2.8769,\n",
      "        2.6255, 2.6450, 2.9651, 2.3015, 3.2637, 3.3986])\n"
     ]
    }
   ],
   "source": [
    "x -=1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使⽤ clone 还有⼀个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外⼀个常⽤的函数就是 item() , 它可以将⼀个标量 Tensor 转换成⼀个Python number："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2649])\n",
      "-0.2648509740829468\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性代数"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "函数      功能\n",
    "trace     对⻆线元素之和(矩阵的迹)\n",
    "diag      对⻆线元素\n",
    "triu/tril 矩阵的上三⻆/下三⻆，可指定偏移量\n",
    "mm/bmm    矩阵乘法，batch的矩阵乘法\n",
    "addmm/addbmm/addmv/addr/badbmm.. 矩阵运算\n",
    "t          转置\n",
    "dot/cross  内积/外积\n",
    "inverse    求逆矩阵\n",
    "svd       奇异值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中的 Tensor ⽀持超过⼀百种操作，包括转置、索引、切⽚、数学运算、线性代数、随机数等\n",
    "等，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 ⼴播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1,3).view(1,2)\n",
    "print(x)\n",
    "y = torch.arange(1,4).view(3,1)\n",
    "print(y)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 x 和 y 分别是1⾏2列和3⾏1列的矩阵，如果要计算 x + y ，那么 x 中第⼀⾏的2个元素被⼴播\n",
    "（复制）到了第⼆⾏和第三⾏，⽽ y 中第⼀列的3个元素被⼴播（复制）到了第⼆列。如此，就可以对2 个3⾏2列的矩阵按元素相加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 运算的内存开销"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前⾯说了，索引、 view 是不会开辟新内存的，⽽像 y = x + y 这样的运算是会新开内存的，然后\n",
    "将 y 指向新内存。为了演示这⼀点，我们可以使⽤Python⾃带的 id 函数：如果两个实例的ID⼀致，那\n",
    "么它们所对应的内存地址相同；反之则不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x  = torch.tensor([1,2])\n",
    "y = torch.tensor([3,4])\n",
    "id_before = id(y)\n",
    "y[:] = y+x\n",
    "print(id(y) == id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想指定结果到原来的 y 的内存，我们可以使⽤前⾯介绍的索引来进⾏替换操作。在下⾯的例⼦中，\n",
    "我们把 x + y 的结果通过 [:] 写进 y 对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2])\n",
    "y = torch.tensor([3,4])\n",
    "id_before = id(y)\n",
    "y[:] = x+y\n",
    "print(id(y) == id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以使⽤运算符全名函数中的 out 参数或者⾃加运算符 += (也即 add_() )达到上述效果，例如\n",
    "torch.add(x, y, out=y) 和 y += x ( y.add_(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2])\n",
    "y = torch.tensor([3,4])\n",
    "id_before = id(y)\n",
    "torch.add(x,y,out = y)# y += x, y.add_(x)\n",
    "print(id(y) == id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 TENSOR 和NUMPY相互转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们很容易⽤ numpy() 和 from_numpy() 将 Tensor 和NumPy中的数组相互转换。但是需要注意的⼀\n",
    "点是： 这两个函数所产⽣的的 Tensor 和NumPy中的数组共享相同的内存（所以他们之间的转换很\n",
    "快），改变其中⼀个时另⼀个也会改变！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有⼀个常⽤的将NumPy中的array转换成 Tensor 的⽅法就是 torch.tensor() , 需要注意的\n",
    "是，此⽅法总是会进⾏数据拷⻉（就会消耗更多的时间和空间），所以返回的 Tensor 和原来的数\n",
    "据不再共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor 转 numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使⽤ numpy() 将 Tensor 转换成NumPy数组:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a +=1\n",
    "print(a,b)\n",
    "b +=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy数组转 Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使⽤ from_numpy() 将NumPy数组转换成 Tensor :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a,b)\n",
    "\n",
    "a+=1\n",
    "print(a,b)\n",
    "b += 1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有在CPU上的 Tensor （除了 CharTensor ）都⽀持与NumPy数组相互转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外上⾯提到还有⼀个常⽤的⽅法就是直接⽤ torch.tensor() 将NumPy数组转换成 Tensor ，需要\n",
    "注意的是该⽅法总是会进⾏数据拷⻉，返回的 Tensor 和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor(a)\n",
    "a +=1 \n",
    "print(a,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 TENSOR ON GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⽤⽅法 to() 可以将 Tensor 在CPU和GPU（需要硬件⽀持）之间相互移动。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#以下代码只有在pytorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.onex_like(x,device = device)\n",
    "\n",
    "x = x.to(device)\n",
    "z  =x+y\n",
    "print(z)\n",
    "print(z.to(\"cpu\",torch.double))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 ⾃动求梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度学习中，我们经常需要对函数求梯度（gradient）。PyTorch提供的autograd 包能够根据输⼊\n",
    "和前向传播过程⾃动构建计算图，并执⾏反向传播。本节将介绍如何使⽤autograd包来进⾏⾃动求梯\n",
    "度的有关操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上⼀节介绍的 Tensor 是这个包的核⼼类，如果将其属性 .requires_grad 设置为 True ，它将开始追\n",
    "踪(track)在其上的所有操作（这样就可以利⽤链式法则进⾏梯度传播了）。完成计算后，可以调\n",
    "⽤ .backward() 来完成所有梯度计算。此 Tensor 的梯度将累积到 .grad 属性中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传⼊任何参数；否则，需要\n",
    "传⼊⼀个与 y 同形的 Tensor 。解释⻅ 2.3.2 节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不想要被继续追踪，可以调⽤ .detach() 将其从追踪记录中分离出来，这样就可以防⽌将来的计\n",
    "算被追踪，这样梯度就传不过去了。此外，还可以⽤ with torch.no_grad() 将不想被追踪的操作代\n",
    "码块包裹起来，这种⽅法在评估模型的时候很常⽤，因为在评估模型时，我们并不需要计算可训练参数\n",
    "（ requires_grad=True ）的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function 是另外⼀个很᯿要的类。 Tensor 和 Function 互相结合就可以构建⼀个记录有整个计算过\n",
    "程的有向⽆环图（DAG）。每个 Tensor 都有⼀个 .grad_fn 属性，该属性即创建该 Tensor 的\n",
    "Function , 就是说该 Tensor 是不是通过某些运算得到的，若是，则 grad_fn 返回⼀个与这些运算相\n",
    "关的对象，否则是None。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 TENSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建⼀个 Tensor 并设置 requires_grad=True :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad = True)\n",
    "print(x)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再做⼀下运算操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x000002787195E198>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意x是直接创建的，所以它没有 grad_fn , ⽽y是通过⼀个加法操作创建的，所以它有⼀个为\n",
    "<AddBackward> 的 grad_fn 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像x这种直接创建的称为叶⼦节点，叶⼦节点对应的 grad_fn 是 None 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x.is_leaf,y.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来点复杂度运算操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 .requires_grad_() 来⽤in-place的⽅式改变 requires_grad 属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000002787195AE80>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2)\n",
    "a = ((a*3)/(a-1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a*a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为 out 是⼀个标量，所以调⽤ backward() 时不需要指定求导变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward() # 等价于 out.backward(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看 out 关于 x 的梯度 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着每⼀次运⾏反向传播，梯度都会累\n",
    "加之前的梯度，所以⼀般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播⼀次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们解释2.3.1节留下的问题，为什么在 y.backward() 时，如果 y 是标量，则不需要\n",
    "为 backward() 传⼊任何参数；否则，需要传⼊⼀个与 y 同形的 Tensor ? 简单来说就是为了避\n",
    "免向量（甚⾄更⾼维张量）对张量求导，⽽转换成标量对张量求导。举个例⼦，假设形状为 m x\n",
    "n 的矩阵 X 经过运算得到了 p x q 的矩阵 Y，Y ⼜经过运算得到了 s x t 的矩阵 Z。那么按照\n",
    "前⾯讲的规则，dZ/dY 应该是⼀个 s x t x p x q 四维张量，dY/dX 是⼀个 p x q x m x n\n",
    "的四维张量。问题来了，怎样反向传播？怎样将两个四维张量相乘？？？这要怎么乘？？？就算能\n",
    "解决两个四维张量怎么乘的问题，四维和三维的张量⼜怎么乘？导数的导数⼜怎么求，这⼀连串的\n",
    "问题，感觉要疯掉…… 为了避免这个问题，我们不允许张量对张量求导，只允许标量对张量求\n",
    "导，求导结果是和⾃变量同形的张量。所以必要时我们要把张量通过将所有张量的元素加权求和的\n",
    "⽅式转换为标量，举个例⼦，假设 y 由 ⾃ 变 量 x 计 算 ⽽ 来 ， w 是 和 y 同 形 的 张 量 ，\n",
    "则 y.backward(w) 的含义是：先计算 l = torch.sum(y * w) ，则 l 是个标量，然后求 l 对\n",
    "⾃变量 x 的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0,4.0],requires_grad = True)\n",
    "y = 2 * x\n",
    "z = y.view(2,2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在 y 不是⼀个标量，所以在调⽤ backward 时需要传⼊⼀个和 y 同形的权᯿向量进⾏加权求和得到\n",
    "⼀个标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.0200, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([[1.0,.01],[0.01,0.001]],dtype = torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意， x.grad 是和 x 同形的张量。\n",
    "再来看看中断梯度追踪的例⼦："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0,requires_grad = True)\n",
    "y1 = x**2\n",
    "with torch.no_grad():\n",
    "    y2 = x**3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1,y1.requires_grad)\n",
    "print(y2,y2.requires_grad)\n",
    "print(y3,y3.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，上⾯的 y2 是没有 grad_fn ⽽且 y2.requires_grad=False 的，⽽ y3 是有 grad_fn\n",
    "的。如果我们将 y3 对 x 求梯度的话会是多少呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "y3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么是2呢？ ，当 时 不应该是5吗？事实上，由于 的定义是\n",
    "被 torch.no_grad(): 包裹的，所以与 有关的梯度是不会回传的，只有与 有关的梯度才会回\n",
    "传，即 对 的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上⾯提到， y2.requires_grad=False ，所以不能调⽤ y2.backward() ，会报错："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，如果我们想要修改 tensor 的数值，但是⼜不希望被 autograd 记录（即不会影响反向传播），\n",
    "那么我么可以对 tensor.data 进⾏操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad = True)\n",
    "print(x.data)#还是一个tensor\n",
    "print(x.data.requires_grad)#但是已经独立于计算图之外\n",
    "\n",
    "y = 2*x\n",
    "x.data *= 100 #只改变了值，不会记录在计算图中，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x)#更改data的值也会影响tensor的值\n",
    "print(x.grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch with fastai",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
